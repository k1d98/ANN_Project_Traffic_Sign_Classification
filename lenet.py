# -*- coding: utf-8 -*-
"""LeNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vhi_s9QWfr6kt75RPE7WSf-_8YAJ5YG7

#Imports
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

import pickle
import os
import csv

import gzip
import numpy as np
import pandas as pd
from time import time

from sklearn.utils import shuffle
from IPython.display import Image
from IPython.display import display
import cv2
import matplotlib.image as mpimg
import tensorflow as tf
import time

import skimage
import skimage as sk
from skimage import transform
from skimage import util
from sklearn.metrics import confusion_matrix

import random
import skimage.morphology as morp
from skimage.filters import rank
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix

from sklearn.model_selection import train_test_split
import keras
import keras.layers as layers
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical
from keras.callbacks import TensorBoard
from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D, Input
from keras.layers import Dense, Dropout, Activation, Flatten
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
from keras.layers import Conv2D, Dense, MaxPool2D, Dropout, Flatten

# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

drive.mount('/content/drive')
drajv = '/content/drive/My Drive/AI/project/';

"""#Data loading"""

training_file = drajv+"tsd/train.p"
validation_file= drajv+"tsd/valid.p"
testing_file = drajv+"tsd/test.p"

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'],valid['labels']
X_test, y_test = test['features'], test['labels']

with open(drajv+'tsd/names.csv', mode='r') as f:
    fread = csv.reader(f)
    next(fread, f)
    sign_labels = [rows[1] for rows in fread]

"""#Data vizualisation"""

n_train = len(X_train)

# TODO: Number of validation examples
n_validation = len(X_valid)

# TODO: Number of testing examples.
n_test = len(X_test)

# TODO: What's the shape of an traffic sign image?
image_shape = X_train[0].shape

# TODO: How many unique classes/labels there are in the dataset.
n_classes = len(set(y_train))

print("Number of training examples =", n_train)
print("Number of validation examples =", n_validation)
print("Number of testing examples =", n_test)
print("Image data shape =", image_shape)
print("Number of classes =", n_classes)

def plot_row_col(rows,cols,X,y):
    plt.figure(figsize = (32,32))
    grid = gridspec.GridSpec(rows,rows)
    # set the spacing between axes.
    grid.update(wspace=0.2, hspace=0.3)  

    for i in range(rows*cols):
        img_plt = plt.subplot(grid[i])
        plt.axis('on')
        img_plt.set_xticklabels([])
        img_plt.set_yticklabels([])
        img_plt.set_aspect('equal')
        index = np.random.randint(0,len(X))
        plt.imshow(X[index].squeeze())
        plt.title(sign_labels[int(y[index])])
#        plt.text(2, 4, str(sign_labels[y[index]]), color='k', backgroundcolor='m')
        plt.axis('off')
    plt.show()
plot_row_col(4,4,X_train,y_train)

# Plot a histogram with the count of each traffic signs in the training set

plt.hist(y_train, bins=n_classes, edgecolor='black', width=0.5)
plt.title('Count of unique traffic signs in the training set')
plt.xlabel('Unique Traffic Sign')
plt.ylabel('Number of samples')
plt.show()

def histogram_plot(dataset, label):
    """
    Plots a histogram of the input data.
        Parameters:
            dataset: Input data to be plotted as a histogram.
            lanel: A string to be used as a label for the histogram.
    """
    hist, bins = np.histogram(dataset, bins=n_classes)
    width = 0.7 * (bins[1] - bins[0])
    center = (bins[:-1] + bins[1:]) / 2
    plt.bar(center, hist, align='center', width=width)
    plt.xlabel(label)
    plt.ylabel("Image count")
    plt.show()
# Plotting histograms of the count of each sign
histogram_plot(y_train, "Training examples")
avg = len(y_train)/n_classes
print('average number of examples for all classes is ', avg)
histogram_plot(y_test, "Testing examples")
histogram_plot(y_valid, "Validation examples")

"""#Preprocessing"""

#GREY IMAGES
#X_train, y_train = shuffle(X_train, y_train)
def gray_scale(image):
    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    return image

def normalize(img):
  normalized = np.array(img,dtype=np.float32)/255
  return np.expand_dims(normalized, axis=-1)

def histo(img, clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(2,2))):
  return clahe.apply(img)

def preprocessing (x, y):
  # Sample images after greyscaling
  gray_images = list(map(gray_scale, x))
  x = gray_images

  #plot_row_col(4, 4, X_train, y_train)
  histoImg = list(map(histo, x))
  x = histoImg
  
  #normalization
  normalized_images = normalize(x)
  x = normalized_images
  
  oldY = y
  y = to_categorical(y, n_classes)
  return x,y,oldY

X_test, y_test,ytest = preprocessing(X_test, y_test)
X_valid, y_valid, yval = preprocessing(X_valid, y_valid)
X_train, y_train, ytren =preprocessing(X_train, y_train)

#Hyperparameters
EPOCHS = 25
BATCH_SIZE = 128
rate = 0.001
dropout = 0.7
augment_num = 800

#Save a copy of the original images
X_train_gold = X_train
X_test_gold = X_test
X_valid_gold = X_valid

#Image augmentation functions
def transform_image(img,ang_range,shear_range,trans_range):
   # random_degree = random.uniform(-25, 25)
   # img =  sk.transform.rotate(img, random_degree)
    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])

    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])

    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2
    
    # Brightness 
    

    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])

    shear_M = cv2.getAffineTransform(pts1,pts2)
        
    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    img = cv2.warpAffine(img,shear_M,(cols,rows))
    return img

def gen_new_images(X_train,y1,n_add,ang_range,shear_range,trans_range):
   
    ## checking that the inputs are the correct lengths
    assert X_train.shape[0] == len(y1)
    # Number of classes: 43
    n_class = len(np.unique(y1))
    X_arr = []
    Y_arr = []
    n_samples = np.bincount(y1)
    for i in range(n_class):
        # Number of samples in each class
      
        if n_samples[i] < n_add:
           # print(i)
            print ("Adding %d samples for class %d" %(n_add-n_samples[i], i))
            for i_n in range(n_add - n_samples[i]):
                before=sum(n_samples[0:i])
                #print(before)
                if(i_n<n_samples[i]):
                  img_trf = transform_image(X_train[before+i_n],ang_range,shear_range,trans_range) 
                  X_arr.append(img_trf)
                  Y_arr.append(y1[before+3])
    X_arr = np.array(X_arr,dtype = np.float32)
    Y_arr = np.array(Y_arr,dtype = np.float32)
    X_arr= np.expand_dims(X_arr, axis=-1)
    return X_arr,Y_arr

print('Color Image shape before augmentation:', X_train.shape)
#Generate new images using the original (not normalized) images
X_train_aug,y_train_aug = gen_new_images(X_train_gold,ytren,augment_num,30,5,5)
## combine the generated images and the original training set
plot_row_col(4,4,X_train_aug,y_train_aug)
print(X_train.shape)
print(X_train_aug.shape)

X_train = np.append(X_train, np.array(X_train_aug), axis=0)
y_train = np.append(ytren, y_train_aug, axis=0)
y_train=to_categorical(y_train)
print('Color Image shape after augmentation:', X_train.shape)

"""#Model LeNet5"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(keras.layers.Dropout(0.2))

model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(keras.layers.Dropout(0.2))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(43, activation='softmax'))

model.build(input_shape=(None,32,32,1))
model.summary()

adam = Adam(lr=5e-4)
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy'])

"""#Model training"""

EPOCHS = 50
BATCH_SIZE = 128

train_generator = ImageDataGenerator().flow(X_train, y_train, batch_size=BATCH_SIZE)
validation_generator = ImageDataGenerator().flow(X_valid, y_valid, batch_size=BATCH_SIZE)

print('# of training images:', X_train.shape[0])
print('# of validation images:', X_valid.shape[0])

steps_per_epoch = X_train.shape[0]//BATCH_SIZE
validation_steps = X_valid.shape[0]//BATCH_SIZE
# construct the training image generator for data augmentation
es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)

with tf.device("/device:GPU:0"):
  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, 
                    validation_data=validation_generator, validation_steps=validation_steps,
                   shuffle=True,callbacks=[es_callback])

"""# Test score"""

score = model.evaluate(X_test, y_test)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""# Confusion matrix"""

pred=model.predict_classes(X_test)
con=confusion_matrix(ytest,pred)

plt.figure(figsize = (20,20))
ax= plt.subplot()
sns.heatmap(con, xticklabels=sign_labels,yticklabels=sign_labels, ax = ax); #annot=True to annotate cells

sign_labels
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix');