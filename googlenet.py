# -*- coding: utf-8 -*-
"""GoogLeNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1auGW9XPzZa3cfoQSzQtw_q_xvXtSJYdy

#Imports
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

import pickle
import os
import csv

import numpy as np
import pandas as pd
import time
from time import time

import random
import cv2
import skimage.morphology as morp
from skimage.filters import rank
from sklearn.utils import shuffle

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import tensorflow as tf

# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import matplotlib.gridspec as gridspec
import skimage

from IPython.display import Image
from IPython.display import display
import matplotlib.image as mpimg

import skimage as sk
from skimage import transform
from skimage import util

from sklearn.utils import shuffle

import keras
import keras.layers as layers
from keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical
from keras.callbacks import TensorBoard
from keras.layers.core import Layer
import keras.backend as K
from keras.models import Model
from keras.layers import Conv2D, MaxPool2D,  \
    Dropout, Dense, Input, concatenate,      \
    GlobalAveragePooling2D, AveragePooling2D,\
    Flatten
from keras.optimizers import SGD 
from keras.callbacks import LearningRateScheduler
from keras.utils import np_utils
import math 



drive.mount('/content/drive')
drajv = '/content/drive/My Drive/AI/project/';

"""#Data loading"""

training_file = drajv+"tsd/train.p"
validation_file= drajv+"tsd/valid.p"
testing_file = drajv+"tsd/test.p"

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']

with open(drajv+'tsd/names.csv', mode='r') as f:
    fread = csv.reader(f)
    next(fread, f)
    sign_labels = [rows[1] for rows in fread]

"""#Data vizualisation"""

n_train = len(X_train)

# TODO: Number of validation examples
n_validation = len(X_valid)

# TODO: Number of testing examples.
n_test = len(X_test)

# TODO: What's the shape of an traffic sign image?
image_shape = X_train[0].shape

# TODO: How many unique classes/labels there are in the dataset.
n_classes = len(set(y_train))

print("Number of training examples =", n_train)
print("Number of validation examples =", n_validation)
print("Number of testing examples =", n_test)
print("Image data shape =", image_shape)
print("Number of classes =", n_classes)

def plot_row_col(rows,cols,X,y):
    plt.figure(figsize = (32,32))
    grid = gridspec.GridSpec(rows,rows)
    # set the spacing between axes.
    grid.update(wspace=0.2, hspace=0.3)  

    for i in range(rows*cols):
        img_plt = plt.subplot(grid[i])
        plt.axis('on')
        img_plt.set_xticklabels([])
        img_plt.set_yticklabels([])
        img_plt.set_aspect('equal')
        index = np.random.randint(0,len(X))
        plt.imshow(X[index].squeeze())
        plt.title(sign_labels[int(y[index])])
        plt.axis('off')
    plt.show()
plot_row_col(4,4,X_train,y_train)

"""#Data preprocessing"""

#GREY IMAGES
def gray_scale(image):
    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    return image

def normalize(img):
  normalized = np.array(img,dtype=np.float32)/255
  return np.expand_dims(normalized, axis=-1)

def histo(img, clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(2,2))):
  return clahe.apply(img)

def histoEq(image):
  kernel = morp.disk(30)
  image = rank.equalize(image, selem=kernel)
  return image

def preprocessing (x, y):
  # Sample images after greyscaling
  gray_images = list(map(gray_scale, x))
  x = gray_images
  #histogram equilizer for images
  histoImg = list(map(histo, x))
  x = histoImg
  
  #normalization
  normalized_images = normalize(x)
  x = normalized_images
  
  oldY = y
  y = to_categorical(y, n_classes)
  return x,y,oldY
X_test, y_test,ytest = preprocessing(X_test, y_test)
X_valid, y_valid, yvalid = preprocessing(X_valid, y_valid)
X_train, y_train, ytren =preprocessing(X_train, y_train)

#Hyperparameters
EPOCHS = 25
BATCH_SIZE = 128
rate = 0.001
dropout = 0.7
augment_num = 800

#Save a copy of the original images
X_train_gold = X_train
X_test_gold = X_test
X_valid_gold = X_valid

#Image augmentation functions
def transform_image(img,ang_range,shear_range,trans_range):
    ang_rot = np.random.uniform(ang_range)-ang_range/2
    rows,cols,ch = img.shape    
    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)

    # Translation
    tr_x = trans_range*np.random.uniform()-trans_range/2
    tr_y = trans_range*np.random.uniform()-trans_range/2
    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])

    # Shear
    pts1 = np.float32([[5,5],[20,5],[5,20]])
    pt1 = 5+shear_range*np.random.uniform()-shear_range/2
    pt2 = 20+shear_range*np.random.uniform()-shear_range/2
    
    # Brightness 
    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])
    shear_M = cv2.getAffineTransform(pts1,pts2)
    img = cv2.warpAffine(img,Rot_M,(cols,rows))
    img = cv2.warpAffine(img,Trans_M,(cols,rows))
    img = cv2.warpAffine(img,shear_M,(cols,rows))
    return img

def gen_new_images(X_train,y1,n_add,ang_range,shear_range,trans_range):
    ## checking that the inputs are the correct lengths
    assert X_train.shape[0] == len(y1)
    # Number of classes: 43
    n_class = len(np.unique(y1))
    X_arr = []
    Y_arr = []
    n_samples = np.bincount(y1)
    for i in range(n_class):
        # Number of samples in each class
        if n_samples[i] < n_add:
           # print(i)
            print ("Adding %d samples for class %d" %(n_add-n_samples[i], i))
            for i_n in range(n_add - n_samples[i]):
                before=sum(n_samples[0:i])
                #print(before)
                if(i_n<n_samples[i]):
                  img_trf = transform_image(X_train[before+i_n],ang_range,shear_range,trans_range) 
                  X_arr.append(img_trf)
                  Y_arr.append(y1[before+3])
    X_arr = np.array(X_arr,dtype = np.float32)
    Y_arr = np.array(Y_arr,dtype = np.float32)
    X_arr= np.expand_dims(X_arr, axis=-1)
    return X_arr,Y_arr

print('Color Image shape before augmentation:', X_train.shape)
#Generate new images using the original (not normalized) images
X_train_aug,y_train_aug = gen_new_images(X_train_gold,ytren,augment_num,30,5,5)
## combine the generated images and the original training set
plot_row_col(4,4,X_train_aug,y_train_aug)
print(X_train.shape)
print(X_train_aug.shape)

X_train = np.append(X_train, np.array(X_train_aug), axis=0)
y_train = np.append(ytren, y_train_aug, axis=0)
y_train=to_categorical(y_train)
print('Color Image shape after augmentation:', X_train.shape)

"""#GoogLeNet"""

dropout = 0.3
def inception_module(x,
                     filters_1x1,
                     filters_3x3_reduce,
                     filters_3x3,
                     filters_5x5_reduce,
                     filters_5x5,
                     filters_pool_proj,
                     name=None):
    
    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    
    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)

    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)

    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)

    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)
    
    return output

kernel_init = keras.initializers.glorot_uniform()
bias_init = keras.initializers.Constant(value=0.2)

input_layer = Input(shape=(32, 32, 1))

x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)
x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)
x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)
x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)
x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)

x = inception_module(x,
                     filters_1x1=64,
                     filters_3x3_reduce=96,
                     filters_3x3=128,
                     filters_5x5_reduce=16,
                     filters_5x5=32,
                     filters_pool_proj=32,
                     name='inception_3a')

x = inception_module(x,
                     filters_1x1=128,
                     filters_3x3_reduce=128,
                     filters_3x3=192,
                     filters_5x5_reduce=32,
                     filters_5x5=96,
                     filters_pool_proj=64,
                     name='inception_3b')

x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)

x = inception_module(x,
                     filters_1x1=192,
                     filters_3x3_reduce=96,
                     filters_3x3=208,
                     filters_5x5_reduce=16,
                     filters_5x5=48,
                     filters_pool_proj=64,
                     name='inception_4a')


x1 = AveragePooling2D((2, 2), strides=1)(x)
x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)
x1 = Flatten()(x1)
x1 = Dense(1024, activation='relu')(x1)
x1 = Dropout(dropout)(x1)
x1 = Dense(43, activation='softmax', name='auxilliary_output_1')(x1)

x = inception_module(x,
                     filters_1x1=160,
                     filters_3x3_reduce=112,
                     filters_3x3=224,
                     filters_5x5_reduce=24,
                     filters_5x5=64,
                     filters_pool_proj=64,
                     name='inception_4b')

x = inception_module(x,
                     filters_1x1=128,
                     filters_3x3_reduce=128,
                     filters_3x3=256,
                     filters_5x5_reduce=24,
                     filters_5x5=64,
                     filters_pool_proj=64,
                     name='inception_4c')

x = inception_module(x,
                     filters_1x1=112,
                     filters_3x3_reduce=144,
                     filters_3x3=288,
                     filters_5x5_reduce=32,
                     filters_5x5=64,
                     filters_pool_proj=64,
                     name='inception_4d')


x2 = AveragePooling2D((2, 2), strides=1)(x)
x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)
x2 = Flatten()(x2)
x2 = Dense(1024, activation='relu')(x2)
x2 = Dropout(dropout)(x2)
x2 = Dense(43, activation='softmax', name='auxilliary_output_2')(x2)

x = inception_module(x,
                     filters_1x1=256,
                     filters_3x3_reduce=160,
                     filters_3x3=320,
                     filters_5x5_reduce=32,
                     filters_5x5=128,
                     filters_pool_proj=128,
                     name='inception_4e')

x = MaxPool2D((2, 2), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)

x = inception_module(x,
                     filters_1x1=256,
                     filters_3x3_reduce=160,
                     filters_3x3=320,
                     filters_5x5_reduce=32,
                     filters_5x5=128,
                     filters_pool_proj=128,
                     name='inception_5a')

x = inception_module(x,
                     filters_1x1=384,
                     filters_3x3_reduce=192,
                     filters_3x3=384,
                     filters_5x5_reduce=48,
                     filters_5x5=128,
                     filters_pool_proj=128,
                     name='inception_5b')

x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)

x = Dropout(dropout)(x)

x = Dense(43, activation='softmax', name='output')(x)

"""#TPU set-up"""

'''%tensorflow_version 2.x
import tensorflow as tf
print("Tensorflow version " + tf.__version__)

try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)'''

"""#Model compiling"""

model = Model(input_layer, [x, x1, x2], name='inception_v1')
model.summary()
epochs = 50
initial_lrate = 0.01

def decay(epoch, steps=100):
    initial_lrate = 0.01
    drop = 0.96
    epochs_drop = 8
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)

lr_sc = LearningRateScheduler(decay, verbose=1)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""#Model training"""

model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])
with tf.device("/device:GPU:0"):
  history = model.fit(X_train, [y_train, y_train, y_train], validation_data=(X_valid, [y_valid, y_valid, y_valid]), epochs=epochs, batch_size=256, callbacks=[lr_sc])

"""#Test score"""

score =model.evaluate(X_test, [y_test, y_test, y_test])
print("Test los:",score[0])
print("Test accuracy",score[4])